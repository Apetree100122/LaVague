
# Installation & set up

In this section, we're going to walk you through how you can install and set up everything you need to run LaVague locally.

> If you just want to test out LaVague without having to install anything locally, you can run our [Quick Tour notebook with Google Colab](https://colab.research.google.com/github/lavague-ai/lavague/blob/main/docs/docs/get-started/quick-tour.ipynb).

To start using LaVague, we need to install a webdriver and store it in our home directory, so that LaVague can interact with the browser in our environment. We will then also need to install the LaVague package.

We provide two key installation options:

- Setting up LaVague in your environment [with our installation script](#local-setup)
- Running LaVague with [the Lavague docker image](#lavague-docker-image)

### Local setup

To set-up LaVague in your local linux environment, you can run our `setup.sh` script:

`sudo bash setup.sh`

To set-up LaVague in your local macos environment, you can run our `setup-macos.sh` script:

`sudo bash setup-macos.sh`

This will perform all necessary steps to set-up LaVague.

> Feel free to inspect the script before running it.
Also I notice you mention managed version of LaVague is also in preparation so that almost nothing needs to be installed for a quick start! (Of course, people can consume/host models to use LaVague)

### LaVague with docker

‚ö†Ô∏è Pre-requisites:

- üêã Docker: Ensure Docker is installed and running on your machine

To use LaVague with our docker image you will first need to:

1. Pull the latest LaVague docker image:

`docker pull lavagueai/lavague:latest`

2. Use `docker run` with the LaVague command of your choice:

`docker run -v /home/$USER/LaVague/lavague-files:/home/vscode/lavague-files -e OPENAI_API_KEY=[YOUR_OPENAI_API_KEY] lavagueai/lavague:latest build`

‚ö†Ô∏è Make sure to replace the `[YOUR_OPENAI_API_KEY]` placeholder with your OpenAI API key.

If you end your docker run command with:

- `build`: it will execute `lavague-build` with the `instructions.txt` and `config.py` files in your `lavague-files` mounted volume
- `launch`: it will execute `lavague-launch` with the `instructions.txt` and `config.py` files in your `lavague-files` mounted volume
- `a custom command`: it will execute this command, for example, `lavague-build --file_path instructions.txt --config_path deepseekcoder.py`

‚ö†Ô∏è In the latter case, make sure you place the relevant files in your mounted `lavague-files` volume!

???+ info "Docker run command in more detail"

    - When using `lavague-build`, we need to add `--user $(id -u):$(id -g)` in order to be able to write our generated automation code to a file in our mounted volume and access them from our host system, we need to match the Docker user and group with the owner of the volume (the host system user).

    - When using `lavague-build`, we need to mount the lavague-files volume with the `-v /home/$USER/LaVague/lavague-files:/home/vscode/lavague-files` option. This is where we can place config files we wish to use with our LaVague commands and where we will find LaVague's output files.

    - When using `lavague-launch`, we need to ensure we bind the host system port 7860 with the container port 7860 by using the `-p 7860:7860` option. This enables us to see the Gradio generated by `lavague-launch` in the container, on localhost:7860 on our local machine.

    - Any environment variables required to run our command, such as your `OPENAI_API_KEY` should be specified with the `-e` option.

### üö® Disclaimer

This project executes LLM-generated code using `exec`. This is not considered a safe practice. We therefore recommend taking extra care when using LaVague (such as running LaVague in a sandboxed environment)!

## Conclusions

You are now ready to use LaVague with the integration of your choice. To see how to do this with our CLI, see our [quick tour](./quick-tour.ipynb) or [integration pages](../integrations/hugging-face-api.ipynb).