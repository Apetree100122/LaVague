{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cUBU_kW8YI"
      },
      "source": [
        "# LaVague 🌊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "LaVague is an open-source framework allowing users to leverage AI to turn natural language instructions into executable code to automate UI actions, such as filling in a form, etc.\n",
        "\n",
        "In this quick tour, we are going to show you step-by-step how can you can set-up and use LaVague to perform a few example actions on webpages. We will create and launch a Gradio demo at the end of the notebook where you can test out using LaVague interactively.\n",
        "\n",
        "> Pre-requisites: Note, if you are running the notebook locally, you will need python (test on python>=3.8) and pip installed.\n",
        "\n",
        "> Note, this notebook uses remote inference with the HuggingFace API. For local inference, see the [local quick-tour](./local-quick-tour.ipynb) (coming soon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing driver for Selenium\n",
        "\n",
        "In this example, we will generate code using [Selenium](https://www.selenium.dev/) to perform user interface actions.\n",
        "\n",
        "Selenium requires a driver to interface with the chosen browser (Chrome, Firefox, etc.)\n",
        "\n",
        "We therefore first need to download the Chrome driver.\n",
        "\n",
        "⚠️ For instructions on how to install a driver for a different browser or instructions for downloading drivers on a different OS, [see the Selenium documentation](https://selenium-python.readthedocs.io/installation.html#drivers)\n",
        "\n",
        "> Note that while we use Selenium for this example. It is possible to achieve the same results using a different automation tool such as Playwright."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20230311ubuntu0.20.04.1).\n",
            "libc6 is already the newest version (2.31-0ubuntu9.14).\n",
            "libcups2 is already the newest version (2.3.1-9ubuntu1.6).\n",
            "libdbus-1-3 is already the newest version (1.12.16-2ubuntu2.3).\n",
            "libexpat1 is already the newest version (2.2.9-1ubuntu0.6).\n",
            "libgbm1 is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
            "libglib2.0-0 is already the newest version (2.64.6-1~ubuntu20.04.6).\n",
            "libnss3 is already the newest version (2:3.49.1-1ubuntu1.9).\n",
            "libstdc++6 is already the newest version (10.5.0-1ubuntu1~20.04).\n",
            "libx11-6 is already the newest version (2:1.6.9-2ubuntu1.6).\n",
            "libx11-xcb1 is already the newest version (2:1.6.9-2ubuntu1.6).\n",
            "unzip is already the newest version (6.0-25ubuntu1.1).\n",
            "xdg-utils is already the newest version (1.1.3-2ubuntu1.20.04.2).\n",
            "libgcc1 is already the newest version (1:10.5.0-1ubuntu1~20.04).\n",
            "libasound2 is already the newest version (1.2.2-2.1ubuntu2.5).\n",
            "libnspr4 is already the newest version (2:4.25-1).\n",
            "libxcb1 is already the newest version (1.14-2).\n",
            "libxext6 is already the newest version (2:1.3.4-0ubuntu1).\n",
            "lsb-release is already the newest version (11.1.0ubuntu2).\n",
            "wget is already the newest version (1.20.3-1ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# If you are missing any apt packages uncomment and run this command first:\n",
        "# !sudo apt update\n",
        "\n",
        "!sudo apt install -y ca-certificates fonts-liberation unzip \\\n",
        "libappindicator3-1 libasound2 libatk-bridge2.0-0 libatk1.0-0 libc6 \\\n",
        "libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgbm1 \\\n",
        "libgcc1 libglib2.0-0 libgtk-3-0 libnspr4 libnss3 libpango-1.0-0 \\\n",
        "libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 \\\n",
        "libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 \\\n",
        "libxrandr2 libxrender1 libxss1 libxtst6 lsb-release wget xdg-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/chrome-for-testing-public/122.0.6261.94/linux64/chrome-linux64.zip\n",
        "!wget https://storage.googleapis.com/chrome-for-testing-public/122.0.6261.94/linux64/chromedriver-linux64.zip\n",
        "!unzip chrome-linux64.zip\n",
        "!unzip chromedriver-linux64.zip\n",
        "!rm chrome-linux64.zip chromedriver-linux64.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing LaVague's Action Engine\n",
        "\n",
        "We provide a PyPi package for LaVague which contains the `ActionEngine` module dedicated to handling all the key AI operations behind the scenes. \n",
        "\n",
        "You can download the PyPi package with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lavague\n",
            "  Using cached lavague-1.0.3-py3-none-any.whl (16 kB)\n",
            "Collecting python-dotenv==1.0.1\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting llama-index-llms-azure-openai==0.1.5\n",
            "  Using cached llama_index_llms_azure_openai-0.1.5-py3-none-any.whl (4.5 kB)\n",
            "Collecting llama-index-embeddings-huggingface==0.1.4\n",
            "  Using cached llama_index_embeddings_huggingface-0.1.4-py3-none-any.whl (7.7 kB)\n",
            "Collecting tree-sitter==0.21.0\n",
            "  Using cached tree_sitter-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (503 kB)\n",
            "Collecting bitsandbytes==0.42.0\n",
            "  Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "Collecting llama-index-llms-huggingface==0.1.4\n",
            "  Using cached llama_index_llms_huggingface-0.1.4-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from lavague) (1.6.0)\n",
            "Processing /home/azureuser/.cache/pip/wheels/29/75/71/9bf68178a74593837f73b6e9d9a070d45d308bddfd2e95290a/google_search_results-2.4.2-py3-none-any.whl\n",
            "Collecting tree-sitter-languages==1.10.2\n",
            "  Using cached tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "Requirement already satisfied: selenium==4.18.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from lavague) (4.18.1)\n",
            "Collecting llama-index-retrievers-bm25==0.1.3\n",
            "  Using cached llama_index_retrievers_bm25-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index==0.10.19\n",
            "  Using cached llama_index-0.10.19-py3-none-any.whl (5.6 kB)\n",
            "Collecting accelerate==0.28.0\n",
            "  Using cached accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "Collecting llama-index-embeddings-azure-openai==0.1.5\n",
            "  Using cached llama_index_embeddings_azure_openai-0.1.5-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-llms-azure-openai==0.1.5->lavague) (0.1.12)\n",
            "Collecting azure-identity<2.0.0,>=1.15.0\n",
            "  Using cached azure_identity-1.15.0-py3-none-any.whl (164 kB)\n",
            "Requirement already satisfied: httpx in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-llms-azure-openai==0.1.5->lavague) (0.27.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-llms-azure-openai==0.1.5->lavague) (0.10.20.post2)\n",
            "Collecting torch<3.0.0,>=2.1.2\n",
            "  Using cached torch-2.2.1-cp38-cp38-manylinux1_x86_64.whl (755.5 MB)\n",
            "Collecting transformers<5.0.0,>=4.37.0\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-embeddings-huggingface==0.1.4->lavague) (0.21.4)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Requirement already satisfied: requests in /home/azureuser/LaVague/lib/python3.8/site-packages (from google-search-results==2.4.2->lavague) (2.31.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /home/azureuser/LaVague/lib/python3.8/site-packages (from selenium==4.18.1->lavague) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /home/azureuser/LaVague/lib/python3.8/site-packages (from selenium==4.18.1->lavague) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from selenium==4.18.1->lavague) (4.10.0)\n",
            "Requirement already satisfied: trio~=0.17 in /home/azureuser/LaVague/lib/python3.8/site-packages (from selenium==4.18.1->lavague) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /home/azureuser/LaVague/lib/python3.8/site-packages (from selenium==4.18.1->lavague) (0.11.1)\n",
            "Collecting rank-bm25<0.3.0,>=0.2.2\n",
            "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.11)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.6)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.9)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.3)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.4)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.9.48)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.5)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index==0.10.19->lavague) (0.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/azureuser/LaVague/lib/python3.8/site-packages (from accelerate==0.28.0->lavague) (1.24.4)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: psutil in /home/azureuser/LaVague/lib/python3.8/site-packages (from accelerate==0.28.0->lavague) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /home/azureuser/LaVague/lib/python3.8/site-packages (from accelerate==0.28.0->lavague) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from accelerate==0.28.0->lavague) (24.0)\n",
            "Collecting azure-core<2.0.0,>=1.23.0\n",
            "  Using cached azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
            "Collecting msal<2.0.0,>=1.24.0\n",
            "  Using cached msal-1.27.0-py2.py3-none-any.whl (101 kB)\n",
            "Collecting msal-extensions<2.0.0,>=0.3.0\n",
            "  Using cached msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
            "Collecting cryptography>=2.5\n",
            "  Using cached cryptography-42.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Requirement already satisfied: sniffio in /home/azureuser/LaVague/lib/python3.8/site-packages (from httpx->llama-index-llms-azure-openai==0.1.5->lavague) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /home/azureuser/LaVague/lib/python3.8/site-packages (from httpx->llama-index-llms-azure-openai==0.1.5->lavague) (1.0.4)\n",
            "Requirement already satisfied: anyio in /home/azureuser/LaVague/lib/python3.8/site-packages (from httpx->llama-index-llms-azure-openai==0.1.5->lavague) (4.3.0)\n",
            "Requirement already satisfied: idna in /home/azureuser/LaVague/lib/python3.8/site-packages (from httpx->llama-index-llms-azure-openai==0.1.5->lavague) (3.6)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (10.2.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (8.2.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2024.3.0)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.14.1)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (4.66.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (0.9.0)\n",
            "Requirement already satisfied: dataclasses-json in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (0.6.4)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (0.6.0)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.2.14)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (3.8.1)\n",
            "Requirement already satisfied: pandas in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2.0.3)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2.0.28)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (0.1.13)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (3.9.3)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Requirement already satisfied: filelock in /home/azureuser/LaVague/lib/python3.8/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface==0.1.4->lavague) (3.13.1)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting triton==2.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.12\"\n",
            "  Using cached triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Requirement already satisfied: jinja2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface==0.1.4->lavague) (3.1.3)\n",
            "Collecting nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/azureuser/LaVague/lib/python3.8/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface==0.1.4->lavague) (2023.12.25)\n",
            "Collecting tokenizers<0.19,>=0.14\n",
            "  Using cached tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Collecting pydantic<2.0,>1.1; python_version == \"3.8\" and extra == \"inference\"\n",
            "  Using cached pydantic-1.10.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from requests->google-search-results==2.4.2->lavague) (3.3.2)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in /home/azureuser/LaVague/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium==4.18.1->lavague) (1.7.1)\n",
            "Requirement already satisfied: sortedcontainers in /home/azureuser/LaVague/lib/python3.8/site-packages (from trio~=0.17->selenium==4.18.1->lavague) (2.4.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from trio~=0.17->selenium==4.18.1->lavague) (23.2.0)\n",
            "Requirement already satisfied: outcome in /home/azureuser/LaVague/lib/python3.8/site-packages (from trio~=0.17->selenium==4.18.1->lavague) (1.3.0.post0)\n",
            "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in /home/azureuser/LaVague/lib/python3.8/site-packages (from trio~=0.17->selenium==4.18.1->lavague) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /home/azureuser/LaVague/lib/python3.8/site-packages (from trio-websocket~=0.9->selenium==4.18.1->lavague) (1.2.0)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (4.1.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (4.12.3)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (0.0.26)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (1.23.26)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (0.0.2)\n",
            "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in /home/azureuser/LaVague/lib/python3.8/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.19->lavague) (0.3.9)\n",
            "Requirement already satisfied: six>=1.11.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.5->lavague) (1.16.0)\n",
            "Collecting PyJWT[crypto]<3,>=1.0.0\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting portalocker<3,>=1.0; platform_system != \"Windows\"\n",
            "  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting cffi>=1.12; platform_python_implementation != \"PyPy\"\n",
            "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
            "\u001b[K     |████████████████████████████████| 444 kB 41.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /home/azureuser/LaVague/lib/python3.8/site-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai==0.1.5->lavague) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.9.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (3.21.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/azureuser/LaVague/lib/python3.8/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.16.0)\n",
            "Requirement already satisfied: joblib in /home/azureuser/LaVague/lib/python3.8/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.3.2)\n",
            "Requirement already satisfied: click in /home/azureuser/LaVague/lib/python3.8/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (2024.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /home/azureuser/LaVague/lib/python3.8/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (3.0.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/azureuser/LaVague/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (6.0.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /home/azureuser/LaVague/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/azureuser/LaVague/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.9.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai==0.1.5->lavague) (1.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/azureuser/LaVague/lib/python3.8/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface==0.1.4->lavague) (2.1.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/azureuser/LaVague/lib/python3.8/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (2.5)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/azureuser/LaVague/lib/python3.8/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->lavague) (1.23.22)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 94.6 MB/s eta 0:00:01\n",
            "\u001b[?25h\u001b[31mERROR: gradio 4.21.0 has requirement pydantic>=2.0, but you'll have pydantic 1.10.14 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: llama-index-llms-huggingface 0.1.4 has requirement huggingface-hub<0.21.0,>=0.20.3, but you'll have huggingface-hub 0.21.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: python-dotenv, azure-core, pycparser, cffi, cryptography, PyJWT, msal, portalocker, msal-extensions, azure-identity, llama-index-llms-azure-openai, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cuda-nvrtc-cu12, nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-nvtx-cu12, nvidia-cusolver-cu12, triton, nvidia-cudnn-cu12, mpmath, sympy, nvidia-nccl-cu12, torch, safetensors, tokenizers, transformers, llama-index-embeddings-huggingface, tree-sitter, scipy, bitsandbytes, llama-index-llms-huggingface, google-search-results, tree-sitter-languages, rank-bm25, llama-index-retrievers-bm25, llama-index, accelerate, llama-index-embeddings-azure-openai, lavague, pydantic\n",
            "  Attempting uninstall: llama-index\n",
            "    Found existing installation: llama-index 0.10.20\n",
            "    Uninstalling llama-index-0.10.20:\n",
            "      Successfully uninstalled llama-index-0.10.20\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "Successfully installed PyJWT-2.8.0 accelerate-0.28.0 azure-core-1.30.1 azure-identity-1.15.0 bitsandbytes-0.42.0 cffi-1.16.0 cryptography-42.0.5 google-search-results-2.4.2 lavague-1.0.3 llama-index-0.10.19 llama-index-embeddings-azure-openai-0.1.5 llama-index-embeddings-huggingface-0.1.4 llama-index-llms-azure-openai-0.1.5 llama-index-llms-huggingface-0.1.4 llama-index-retrievers-bm25-0.1.3 mpmath-1.3.0 msal-1.27.0 msal-extensions-1.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 pycparser-2.21 pydantic-1.10.14 python-dotenv-1.0.1 rank-bm25-0.2.2 safetensors-0.4.2 scipy-1.10.1 sympy-1.12 tokenizers-0.15.2 torch-2.2.1 transformers-4.38.2 tree-sitter-0.21.0 tree-sitter-languages-1.10.2 triton-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install lavague"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cu4DWoApeOl"
      },
      "source": [
        "### Installing other PyPi dependencies\n",
        "\n",
        "We also need to install`Gradio`, which we will use to interact to quickly build an interactive example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace set-up\n",
        "\n",
        "⚠️ For remote inference with Hugging Face inference api, you will need to provide your HuggingFace API token.\n",
        "\n",
        "Alternatively, you can run the notebook entirely locally (the model will be downloaded and run locally instead of via an API) with our [local quick-tour](./local-quick-tour.ipynb) (coming soon).\n",
        "\n",
        "> A HuggingFace API token enables you to interact with models hosted by HuggingFace. If you don't have one, you will need to create a HuggingFace account and create one as detailed [here](https://huggingface.co/docs/hub/en/security-tokens).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e5k-Tw5EVZuA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building our Gradio demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing the webdriver\n",
        "\n",
        "First of all, we configure our webdriver settings to suite our use case (creating a Gradio demo).\n",
        "\n",
        "> - We use `headless` mode to turn of the webdriver GUI as we want to perform these tasks in the background since we will use Gradio for our visual display for this demo.\n",
        "> - We turn off the Chrome `sandbox` security feature which restricts the browser's access to the system it's running on so that we can share this quick-tour as a Google Colab notebook.\n",
        "> - We set the `window-size` for the screenshots we will later capture to show the user the before/after results of our demo queries.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium import webdriver\n",
        "\n",
        "## Setup chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\") # Ensure GUI is off\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--window-size=1600,900\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set the path to the ChromeDriver and initialize it, passing it the config options we just defined.\n",
        "\n",
        "> If you are running the notebook locally and change the location of the driver, you will need to update the path here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from lavague.ActionEngine import ActionEngine\n",
        "from lavague.defaults import DefaultLocalLLM, DefaultLLM\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "import re\n",
        "\n",
        "# Set path to chrome/chromedriver as per your configuration\n",
        "try:\n",
        "    import google.colab\n",
        "    chrome_options.binary_location = \"/content/chrome-linux64/chrome\"\n",
        "    webdriver_service = Service(\"/content/chromedriver-linux64/chromedriver\")\n",
        "except:\n",
        "    import os.path\n",
        "    homedir = os.path.expanduser(\".\")\n",
        "    print(homedir)\n",
        "    chrome_options.binary_location = f\"chrome-linux64/chrome\"\n",
        "    webdriver_service = Service(f\"chromedriver-linux64/chromedriver\")\n",
        "\n",
        "# Initialize Chrome Browser driver\n",
        "driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing ActionEngine with our chosen LLM\n",
        "\n",
        "Next, we will initialize our ActionEngine which will perform all the key AI operations needed to generate the Selenium code to perform the desired action.\n",
        "\n",
        "We will leverage the default `LLM` (Nous-Hermes-2-Mixtral-8x7B-DPO) and `embedding` (bge-small-en-v1.5) options. However, you can pass any LlamaIndex LLM or embedding model to the constructor at this point.\n",
        "\n",
        "> To use our default local LLM, you can pass our `DefaultLocalLLM()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lavague.ActionEngine import ActionEngine\n",
        "from lavague.defaults import DefaultLocalLLM, DefaultLLM\n",
        "\n",
        "action_engine = ActionEngine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The demo code\n",
        "\n",
        "The demo code includes several auxiliary functions which we will not cover in detail, but one key function to understand is the `process_instructions` function. \n",
        "\n",
        "This function takes the user natural language instructions and website URL and returns AI-generated Selenium Python code needed to perform the desired action on the website.\n",
        "\n",
        "The function first gets the HTML source code for the website we wish to perform our web workflow on. \n",
        "\n",
        "Next, it calls the `get_query_engine()` method which breaks down the HTML document into smaller, more manageable chunks, indexes them and retrieves the most relevant chunks HTML of code.\n",
        "\n",
        "Then, it calls the `query_engine.query()` method to perform inference. This Action Engine method inserts the user’s instructions and the most relevant pieces of the HTML source code into our constructed prompt template and uses this to query the LLM. \n",
        "\n",
        "It returns the LLM response, aka. the generated Selenium code to perform the user's desired action, as well as the source nodes (chunks of HTML source code) in generating this response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_CHARS = 1500\n",
        "\n",
        "def process_instruction(instructions, url_input):\n",
        "    if url_input != driver.current_url:\n",
        "        driver.get(url_input)\n",
        "    source_code = driver.page_source\n",
        "    query_engine = action_engine.get_query_engine(source_code)\n",
        "    response = query_engine.query(instructions)\n",
        "    source_nodes = response.get_formatted_sources(MAX_CHARS)\n",
        "    return response.response, source_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following `exec_code` method, will extract the Python Selenium code generated by the LLM, to ensure there is no other values first and then execute the code to perform the desired action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_first_python_code(markdown_text):\n",
        "    # Pattern to match the first ```python ``` code block\n",
        "    pattern = r\"```python(.*?)```\"\n",
        "    \n",
        "    # Using re.DOTALL to make '.' match also newlines\n",
        "    match = re.search(pattern, markdown_text, re.DOTALL)\n",
        "    if match:\n",
        "        # Return the first matched group, which is the code inside the ```python ```\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        # Return None if no match is found\n",
        "        return None\n",
        "\n",
        "def exec_code(code, source_nodes, full_code):\n",
        "    code = extract_first_python_code(code)\n",
        "    html = driver.page_source\n",
        "    try:\n",
        "        exec(code)\n",
        "        output = \"Successful code execution\"\n",
        "        status = \"\"\"<p style=\"color: green; font-size: 20px; font-weight: bold;\">Success!</p>\"\"\"\n",
        "        full_code += code\n",
        "    except Exception as e:\n",
        "        output = f\"Error in code execution: {str(e)}\"\n",
        "        status = \"\"\"<p style=\"color: red; font-size: 20px; font-weight: bold;\">Failure! Open the Debug tab for more information</p>\"\"\"\n",
        "    return output, code, html, status, full_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following methods are used to get screenshots of the website which we are performing our automated action on to show before/after images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "e5k-Tw5EVZuA"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def process_url(url):\n",
        "    driver.get(url)\n",
        "    driver.save_screenshot(\"screenshot.png\")\n",
        "    return \"screenshot.png\"\n",
        "\n",
        "def update_image_display(img):\n",
        "    driver.save_screenshot(\"screenshot.png\")\n",
        "    url = driver.current_url\n",
        "    return \"screenshot.png\", url\n",
        "\n",
        "def update_image_display(img):\n",
        "    driver.save_screenshot(\"screenshot.png\")\n",
        "    url = driver.current_url\n",
        "    return \"screenshot.png\", url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the code sets up and launches our Gradio demo. It sets up the visual elements of the Gradio demo and executes the above functions as per user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "title = \"\"\"\n",
        "<div align=\"center\">\n",
        "  <h1>🌊 Welcome to LaVague</h1>\n",
        "  <p>Redefining internet surfing by transforming natural language instructions into seamless browser interactions.</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "def show_processing_message():\n",
        "    return \"Processing...\"\n",
        "\n",
        "def create_demo(base_url, instructions):\n",
        "  with gr.Blocks() as demo:\n",
        "      with gr.Tab(\"LaVague\"):\n",
        "        with gr.Row():\n",
        "            gr.HTML(title)\n",
        "        with gr.Row():\n",
        "            url_input = gr.Textbox(value=base_url, label=\"Enter URL and press 'Enter' to load the page.\")\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=7):\n",
        "                image_display = gr.Image(label=\"Browser\", interactive=False)\n",
        "            \n",
        "            with gr.Column(scale=3):\n",
        "                with gr.Accordion(label=\"Full code\", open=False):\n",
        "                    full_code = gr.Code(value=\"\", language=\"python\", interactive=False)\n",
        "                code_display = gr.Code(label=\"Generated code\", language=\"python\",\n",
        "                                        lines=5, interactive=True)\n",
        "                \n",
        "                status_html = gr.HTML()\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=8):\n",
        "                text_area = gr.Textbox(label=\"Enter instructions and press 'Enter' to generate code.\")\n",
        "                gr.Examples(examples=instructions, inputs=text_area)\n",
        "      with gr.Tab(\"Debug\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                log_display = gr.Textbox(interactive=False, lines=20)\n",
        "            with gr.Column():\n",
        "                source_display = gr.Code(language=\"html\", label=\"Retrieved nodes\", interactive=False, lines=20)\n",
        "        with gr.Row():\n",
        "            with gr.Accordion(label=\"Full HTML\", open=False):\n",
        "                full_html = gr.Code(language=\"html\", label=\"Full HTML\", interactive=False, lines=20)\n",
        "  \n",
        "      # Linking components\n",
        "      url_input.submit(process_url, inputs=url_input, outputs=image_display)\n",
        "      text_area.submit(show_processing_message, outputs=[status_html]).then(\n",
        "          process_instruction, inputs=[text_area, url_input], outputs=[code_display, source_display]\n",
        "          ).then(\n",
        "          exec_code, inputs=[code_display, source_display, full_code], \n",
        "          outputs=[log_display, code_display, full_html, status_html, full_code]\n",
        "      ).then(\n",
        "          update_image_display, inputs=image_display, outputs=[image_display, url_input]\n",
        "      )\n",
        "  demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now try the demo where we use natural language instructions to automate an action on the Hugging Face website.\n",
        "\n",
        "⚠️ You will need to interact with these examples, by clicking on the URL and pressing enter, and then selecting your chosen natural language instruction in the Gradio interface, and again clicking on the chosen instruction and pressing enter. The action should then be visibly executed in the visual interface.\n",
        "\n",
        "> Note you can open the Gradio interface in your browser using the URL displayed in the cell output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "9kPolvYHfFmA",
        "outputId": "828680f7-bd0e-456c-ec87-a308a0bb5454"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://huggingface.co/\"\n",
        "\n",
        "instructions = [\"Click on the Datasets item on the menu, between Models and Spaces\",\n",
        "                \"Click on the search bar 'Filter by name', type 'The Stack', and press 'Enter'\",\n",
        "                \"Scroll by 500 pixels\",]\n",
        "\n",
        "create_demo(base_url, instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below you can explore a second example with the IRS website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "MrQ82tgr4_zt",
        "outputId": "0c1f0678-93cd-4cbf-db75-88d9988cdaf8"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://www.irs.gov/\"\n",
        "\n",
        "instructions = [\"Click on the 'Pay' item on the menu, between 'File' and 'Refunds'\",\n",
        "                \"Click on 'Pay Now with Direct Pay' just below 'Pay from your Bank Account'\",\n",
        "                \"Click on 'Make a Payment', just above 'Answers to common questions'\",]\n",
        "\n",
        "create_demo(base_url, instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That brings us to the end of this quick-tour. If you have any questions, join us on the LaVague Discord [here](https://discord.com/invite/SDxn9KpqX9)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d9126c291a04e8fb1ce30a7103197d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c0d6723eba4298b61ddd67c1595f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d9126c291a04e8fb1ce30a7103197d1",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a36c6df55813490a9e0b42b67a698327",
            "value": 4
          }
        },
        "2600f072828a4abf8e8ed51cd64f2dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_984f401572a645f7861db035993c806b",
            "placeholder": "​",
            "style": "IPY_MODEL_2ba0655ceb93417d859921bf08b5ec9d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2ba0655ceb93417d859921bf08b5ec9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "398c2359c137457b94fe7ed7b8c4b939": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2600f072828a4abf8e8ed51cd64f2dc2",
              "IPY_MODEL_15c0d6723eba4298b61ddd67c1595f66",
              "IPY_MODEL_d1a4215edd20433482d339c83c9d0060"
            ],
            "layout": "IPY_MODEL_c71a506bf63948c8bfb79b6811705de5"
          }
        },
        "777ac9bad3974a2ab7062644bda6a3c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "984f401572a645f7861db035993c806b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36c6df55813490a9e0b42b67a698327": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c71a506bf63948c8bfb79b6811705de5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a4215edd20433482d339c83c9d0060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec5d092ea24946279f14f03570a48c98",
            "placeholder": "​",
            "style": "IPY_MODEL_777ac9bad3974a2ab7062644bda6a3c6",
            "value": " 4/4 [00:12&lt;00:00,  2.89s/it]"
          }
        },
        "ec5d092ea24946279f14f03570a48c98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
